{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from os import path\n",
    "import re\n",
    "\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "fd = ['..','data','raw']\n",
    "\n",
    "data = {}\n",
    "fn_list = ['orders.csv', 'products.csv', 'order_products__prior.csv', 'order_products__train.csv', 'departments.csv', 'aisles.csv']\n",
    "\n",
    "for fn in fn_list:\n",
    "    fp = path.join(*fd, fn)\n",
    "\n",
    "    with open(file=fp, mode='r', encoding='utf8') as file:\n",
    "        import re\n",
    "        label = re.sub('\\.csv$', '', fn)\n",
    "        data[label] = pd.read_csv(file, encoding='utf8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prior    3214874\n",
       "train     131209\n",
       "test       75000\n",
       "Name: eval_set, dtype: int64"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['orders']['eval_set'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naming conventions as per the Kaggle description are as follows:\n",
    "- Prior: historic order data to be used in training models.\n",
    "- Train: historic order data to be use in evaluating trained models.\n",
    "- Test: \"new\" orders on which to make recommendations to submit to the competition for ultimate performance scoring. Data on products ordered for these orders is not provided.\n",
    "\n",
    "Since this project is not actually participating in submission of predictions with the Kaggle competition (which has finished), we will simply focus on the \"prior\" and \"train\" sets as our train and test sets, respectively.\n",
    "\n",
    "This assumes we take a simple approach of evaluating recommendations by their ability to predict items that will be in the set of test orders. If we shift to using an alternative evaluation method/metric, then this split approach may need to be revisited.\n",
    "\n",
    "For the time being, we will create full datasets (joining all tables) for our training (\"prior\") and testing (\"train\") sets for convenience in future work and analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = pd.merge(data['orders'], data['order_products__prior'], on='order_id')\\\n",
    "               .merge(data['products'].merge(data['departments'], on='department_id').merge(data['aisles'], on='aisle_id'), on='product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(32434489, 15)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "prior    32434489\n",
       "Name: eval_set, dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_train['eval_set'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_test = pd.merge(data['orders'], data['order_products__train'], on='order_id')\\\n",
    "               .merge(data['products'].merge(data['departments'], on='department_id').merge(data['aisles'], on='aisle_id'), on='product_id')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1384617, 15)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "train    1384617\n",
       "Name: eval_set, dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_test['eval_set'].value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will save these datasets to the interim folder for now in case anything changes in our approach or further manipulation is done in terms of cleaning and feature engineering."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pickle file for train data\n",
    "\n",
    "f = 'train.p'\n",
    "d = '../data/interim'\n",
    "fp = path.join(d,f)\n",
    "\n",
    "with open(fp, 'wb') as file:\n",
    "    pickle.dump(data_train, file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create pickle file for test data\n",
    "\n",
    "f = 'test.p'\n",
    "d = '../data/interim'\n",
    "fp = path.join(d,f)\n",
    "\n",
    "with open(fp, 'wb') as file:\n",
    "    pickle.dump(data_test, file)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
