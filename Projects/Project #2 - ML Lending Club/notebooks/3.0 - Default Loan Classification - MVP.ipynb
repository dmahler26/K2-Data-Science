{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "pd.set_option('display.max_columns', None, 'display.max_rows', 100) \n",
    "\n",
    "from os import path\n",
    "import pickle\n",
    "\n",
    "import sys\n",
    "sys.path.append('..')\n",
    "import src.functions.my_functions as my_func\n",
    "\n",
    "import missingno as msno\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.colors as colors\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "from sklearn.pipeline import Pipeline, FeatureUnion\n",
    "\n",
    "from sklearn.preprocessing import StandardScaler, Normalizer\n",
    "from sklearn.model_selection import train_test_split, GridSearchCV, cross_val_score, RandomizedSearchCV, cross_val_predict, learning_curve\n",
    "from sklearn.metrics.scorer import make_scorer\n",
    "\n",
    "from sklearn.metrics import confusion_matrix, f1_score, recall_score, precision_score, precision_recall_curve, fbeta_score\n",
    "\n",
    "from sklearn.dummy import DummyClassifier\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load train set\n",
    "f = 'loan_train.p'\n",
    "d = ['..', 'data', 'processed']\n",
    "fp = path.join(*d, f)\n",
    "\n",
    "with open(fp, 'rb') as file:\n",
    "    train = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load train target set\n",
    "f = 'loan_train_target.p'\n",
    "d = ['..', 'data', 'processed']\n",
    "fp = path.join(*d, f)\n",
    "\n",
    "with open(fp, 'rb') as file:\n",
    "    train_target = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load test set\n",
    "f = 'loan_test.p'\n",
    "d = ['..', 'data', 'processed']\n",
    "fp = path.join(*d, f)\n",
    "\n",
    "with open(fp, 'rb') as file:\n",
    "    test = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load test target set\n",
    "f = 'loan_test_target.p'\n",
    "d = ['..', 'data', 'processed']\n",
    "fp = path.join(*d, f)\n",
    "\n",
    "with open(fp, 'rb') as file:\n",
    "    test_target = pickle.load(file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((152218, 47), (152218, 2))"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train.shape, train_target.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38055, 47), (38055, 2))"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test.shape, test_target.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Preperation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Columns identified thus far as best for classification (during data prep, select K best)\n",
    "num_attr = ['funded_amnt_q10', 'int_rate_delta', 'annual_inc_q10', 'dti', 'delinq_2yrs', 'inq_last_6mths',\n",
    "            'open_acc', 'revol_bal_log', 'revol_util', 'total_acc', 'collections_12_mths_ex_med',\n",
    "            'acc_now_delinq', 'rev_lim_sqrt', 'tot_cur_bal', 'tot_coll_amt', 'subgrade_p_value',\n",
    "            'lti', 'rbti', 'tbti', 'cr_line_td_log', 'emp_length_val']\n",
    "\n",
    "bin_attr = ['had_delinq', 'had_major_derog', 'had_record', 'verified', 'term_bin']\n",
    "\n",
    "cat_attr = ['purpose', 'home_ownership']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_prep = Pipeline([('custom', my_func.CustomNumAttributes()),\n",
    "                     ('select', my_func.DataFrame_Selector(num_attr)), # Select num columns\n",
    "                     ('sc', StandardScaler())]) # Scale data\n",
    "\n",
    "bin_prep = Pipeline([('custom', my_func.CustomBinAttributes()),\n",
    "                     ('select', my_func.DataFrame_Selector(bin_attr))]) # Select binary columns\n",
    "\n",
    "cat_prep = Pipeline([('encode', my_func.DataFrame_DummyEncoder(cat_attr))]) # Select & encode categrocial columns\n",
    "\n",
    "feature_prep = FeatureUnion([('num', num_prep),\n",
    "                             ('bin', bin_prep),\n",
    "                             ('cat', cat_prep)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_full = feature_prep.fit_transform(train)\n",
    "y_train_full = train_target['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_test = feature_prep.transform(test)\n",
    "y_test = test_target['default']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((152218, 44), (152218,))"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train_full.shape, y_train_full.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((38055, 44), (38055,))"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape, y_test.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will split the training set further into a training and validation set:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_val, y_train, y_val = train_test_split(X_train_full, y_train_full, test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((121774, 44), (121774,))"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape, y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((30444, 44), (30444,))"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape, y_val.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Modeling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Initialize dict for models\n",
    "models = {}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classifier_summary(y_actual, y_pred, print_results=True): \n",
    "    \n",
    "    #f1 = f1_score(y_actual, y_pred)\n",
    "    f2 = fbeta_score(y_actual, y_pred, beta=2)\n",
    "    recall = recall_score(y_actual, y_pred)\n",
    "    precision = precision_score(y_actual, y_pred)\n",
    "    \n",
    "    conf_mat = confusion_matrix(y_actual, y_pred)\n",
    "    \n",
    "    specificity = conf_mat[0,0] / (conf_mat[0,:].sum())\n",
    "    fallout = 1 - specificity\n",
    "    precision_neg = conf_mat[0,0] / (conf_mat[:,0].sum())\n",
    "    \n",
    "    df_cmat = pd.DataFrame(conf_mat).rename(index={0:'Actual Negative', 1:'Actual Positive'},\n",
    "                                  columns={0:'Predicted Negative', 1:'Predicted Positive'})\n",
    "    \n",
    "    df_scores = pd.DataFrame([{'Rate': 'F2', 'Score': f2},\n",
    "                              {'Rate': 'Recall', 'Score': recall},\n",
    "                              {'Rate': 'Precision (pos)', 'Score': precision},\n",
    "                              {'Rate': 'Precision (neg)', 'Score': precision_neg},\n",
    "                              {'Rate': 'Specificity', 'Score': specificity}]).set_index('Rate')\n",
    "    \n",
    "    if print_results:\n",
    "        print('Confusion Matrix:')\n",
    "        print(df_cmat)\n",
    "        print(20*'-')\n",
    "        print('Accuracy Scores:')\n",
    "        print(df_scores)\n",
    "   \n",
    "    return df_cmat, df_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gs_score_summary(gs):\n",
    "    scores = gs.scoring\n",
    "    print('-'*20)\n",
    "    for score in scoring:\n",
    "        i = np.argmin(gs.cv_results_['rank_test_' + str(score)])\n",
    "        print('Best {}:'.format(score.title()))\n",
    "        print('Params: {}'.format(gs.cv_results_['params'][i]))\n",
    "\n",
    "        for s in scores:\n",
    "            print('{} = {}'.format(s.title(), gs.cv_results_['mean_test_'+str(s)][i]))\n",
    "        print('-'*20)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_cvs(cvs, scoring='CV'):\n",
    "    print('Mean {} score = {:.3f} (+\\- {:.3f})'.format(scoring, cvs.mean(), cvs.std()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Measuring Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The business problem at hand for this classification problem is to identify loans that will default. In terms of measuring the performance of our models, the question arises as to which method of scoring / accuracy we value most. In the context of investing in loans, the risks and consequences of failing to identify a default loan greatly outweight those of accidentally discarding some quality loans as default. With this premise, recall (i.e. the proportion of actual default loans identified) should be score we seek to maximize.\n",
    "\n",
    "However, attempts to boost recall will inevitabely reduce model precision. In an extreme yet possible example, a model that is optimized to identify 90% of default loans could come at the cost of discarding 90% of non-default loans (i.e. 50% precision). From a business standpoint this would leave an unreasonable number of viable loans in the pool of non-default predictions. Consequently, a better approach for optimizing our models is to use an F beta scoring in which recall is given more weight than precision (i.e. beta > 1).\n",
    "\n",
    "We will proceed with using F2 scoring (beta = 2), in which recall is essentially valued twice as much as precision. This should hopefully allow to us to still maximize recall to a certain degree without decreasing precision to unnacceptable levels."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "f2_score = make_scorer(fbeta_score, beta=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy classifier (stratified):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(constant=None, random_state=None, strategy='stratified')"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_strat = DummyClassifier(strategy='stratified')\n",
    "dummy_strat.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_strat_train_pred = dummy_strat.predict(X_train)\n",
    "dummy_strat_val_pred = dummy_strat.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               79333               18981\n",
      "Actual Positive               18952                4508\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.192109\n",
      "Recall           0.192157\n",
      "Precision (pos)  0.191920\n",
      "Precision (neg)  0.807173\n",
      "Specificity      0.806935\n"
     ]
    }
   ],
   "source": [
    "my_func.classifier_summary(y_train, dummy_strat_train_pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               19842                4788\n",
      "Actual Positive                4691                1123\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.192512\n",
      "Recall           0.193154\n",
      "Precision (pos)  0.189985\n",
      "Precision (neg)  0.808788\n",
      "Specificity      0.805603\n"
     ]
    }
   ],
   "source": [
    "my_func.classifier_summary(y_val, dummy_strat_val_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dummy classifier (unfirom):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DummyClassifier(constant=None, random_state=None, strategy='uniform')"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dummy_uniform = DummyClassifier(strategy='uniform')\n",
    "dummy_uniform.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dummy_uniform_train_pred = dummy_uniform.predict(X_train)\n",
    "dummy_uniform_val_pred = dummy_uniform.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               49269               49045\n",
      "Actual Positive               11780               11680\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.377835\n",
      "Recall           0.497869\n",
      "Precision (pos)  0.192343\n",
      "Precision (neg)  0.807040\n",
      "Specificity      0.501139\n"
     ]
    }
   ],
   "source": [
    "my_func.classifier_summary(y_train, dummy_uniform_train_pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               12247               12383\n",
      "Actual Positive                2988                2826\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.367347\n",
      "Recall           0.486068\n",
      "Precision (pos)  0.185811\n",
      "Precision (neg)  0.803873\n",
      "Specificity      0.497239\n"
     ]
    }
   ],
   "source": [
    "my_func.classifier_summary(y_val, dummy_uniform_val_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These two dummy classifiers reperesent opposite extremes of our modelling potential. The stratified predictions predict defaults infrequently due to its small class proportion, resulting in high specificity but very low recall & precisons scores.\n",
    "\n",
    "The uniform predictor, on the other hand, boosts the frequency of predicted default loans resulting in a much better recall but still with low precision.\n",
    "\n",
    "Due to the higher recall scores with our uniform predictor we see a significant improvement in the F2 score. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another baseline model which should provide a better compromise between these two extremes is a one rule classifier (single level decision tree):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_r = DecisionTreeClassifier(max_depth=1, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight='balanced', criterion='gini', max_depth=1,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "one_r.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_r_f2_cvs = cross_val_score(estimator=one_r, X=X_train, y=y_train, cv=5, scoring=f2_score, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_r_recall_cvs = cross_val_score(estimator=one_r, X=X_train, y=y_train, cv=5, scoring='recall', n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f2 score = 0.525 (+/- 0.010)\n",
      "Mean recall score = 0.680 (+/- 0.028)\n"
     ]
    }
   ],
   "source": [
    "my_func.print_cvs(one_r_f2_cvs, 'f2')\n",
    "my_func.print_cvs(one_r_recall_cvs, 'recall')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "one_r_train_pred = one_r.predict(X_train)\n",
    "one_r_val_pred = one_r.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               58462               39852\n",
      "Actual Positive                7996               15464\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.518383\n",
      "Recall           0.659165\n",
      "Precision (pos)  0.279557\n",
      "Precision (neg)  0.879683\n",
      "Specificity      0.594646\n"
     ]
    }
   ],
   "source": [
    "my_func.classifier_summary(y_train, one_r_train_pred);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               14564               10066\n",
      "Actual Positive                2017                3797\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.511463\n",
      "Recall           0.653079\n",
      "Precision (pos)  0.273895\n",
      "Precision (neg)  0.878355\n",
      "Specificity      0.591311\n"
     ]
    }
   ],
   "source": [
    "my_func.classifier_summary(y_val, one_r_val_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This OneR model provides a more balanced baseline to compare model performance with. We can establish the following standards for deciding which models to pursue:\n",
    "\n",
    "- F2 score above 0.5\n",
    "- Recall greater than 0.60 (i.e. able to identify at least 60% of defaulted loans)\n",
    "- Specificity greather than 0.5 (i.e. retain at least 50% of healthy loans)\n",
    "- Precision greather than 0.25 (lower priority due to class imbalance)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Logistic Regression\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Basic model, no tuning:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.07 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "lr = LogisticRegression()\n",
    "\n",
    "lr_timeit = %timeit -n1 -r1 -o \\\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not a bad fit time with 4 seconds. Let's see how the base model performs with cross validation scores (f1 and recall):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_f2_cvs = cross_val_score(estimator=lr,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=5,\n",
    "                             scoring=f2_score,\n",
    "                             n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f2 score = 0.070 (+\\- 0.004)\n"
     ]
    }
   ],
   "source": [
    "my_func.print_cvs(lr_f2_cvs, 'f2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_train_pred = lr.predict(X_train)\n",
    "lr_val_pred = lr.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               97011                1303\n",
      "Actual Positive               22103                1357\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.070311\n",
      "Recall           0.057843\n",
      "Precision (pos)  0.510150\n",
      "Precision (neg)  0.814438\n",
      "Specificity      0.986747\n"
     ]
    }
   ],
   "source": [
    "lr_train_summary = my_func.classifier_summary(y_train, lr_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               24324                 306\n",
      "Actual Positive                5478                 336\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.070299\n",
      "Recall           0.057792\n",
      "Precision (pos)  0.523364\n",
      "Precision (neg)  0.816187\n",
      "Specificity      0.987576\n"
     ]
    }
   ],
   "source": [
    "lr_val_summary = my_func.classifier_summary(y_val, lr_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our recall (and F2) score of interest is in dire need of improvement since we are only identifying 5% of the actual defaulted loans. It is likely that this model is suffering from the class imabalance between default and non-default loans, so we will see if weighing class accordingly provides any improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.88 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "lr_bal = LogisticRegression(class_weight='balanced')\n",
    "\n",
    "lr_bal_timeit = %timeit -n1 -r1 -o \\\n",
    "lr_bal.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_bal_f2_cvs = cross_val_score(estimator=lr_bal,\n",
    "                             X=X_train,\n",
    "                             y=y_train,\n",
    "                             cv=5,\n",
    "                             scoring=f2_score,\n",
    "                             n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f2 score = 0.533 (+\\- 0.004)\n"
     ]
    }
   ],
   "source": [
    "my_func.print_cvs(lr_bal_f2_cvs, 'f2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lrb_train_pred = lr_bal.predict(X_train)\n",
    "lrb_val_pred = lr_bal.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               64171               34143\n",
      "Actual Positive                8167               15293\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.533690\n",
      "Recall           0.651876\n",
      "Precision (pos)  0.309349\n",
      "Precision (neg)  0.887099\n",
      "Specificity      0.652715\n"
     ]
    }
   ],
   "source": [
    "lrb_train_summary = my_func.classifier_summary(y_train, lrb_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               16135                8495\n",
      "Actual Positive                2021                3793\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.533564\n",
      "Recall           0.652391\n",
      "Precision (pos)  0.308675\n",
      "Precision (neg)  0.888687\n",
      "Specificity      0.655095\n"
     ]
    }
   ],
   "source": [
    "lrb_val_summary = my_func.classifier_summary(y_val, lrb_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Balancing the class weights yields significant improvements to the recall & F2 score, as the increased weight for our default class leads to more loans being flagged as such. However, the consequence is that precision is decreased considerably - from 0.5 to 0.3. This means that with this approach, approx. two thirds of the loans identified as default are in fact false positives. However, our false negative rate (denoted 'Precision (neg)') actually improves slightly, meaning the loans that remain as predicted negative have a higher guarantee of actually being non-default. Specificty has decreased considerably due to a larger number of false positives, but is still better than our baseline OneR score of 0.6.\n",
    "\n",
    "All in all given the comparable F2 score and improved specificty and precision over the OneR model, logistic regression shows promise as a model worth exploring and tuning further."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lr_bal.coef_f_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Naive Bayes Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "gnb_time = %timeit -n1 -r1 -o \\\n",
    "gnb.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb_f2_cvs = cross_val_score(estimator=gnb, X=X_train, y=y_train, cv=5, scoring=f2_score, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f2 score = 0.410 (+\\- 0.030)\n"
     ]
    }
   ],
   "source": [
    "my_func.print_cvs(gnb_f2_cvs, 'f2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb_train_pred = gnb.predict(X_train)\n",
    "gnb_val_pred = gnb.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               78143               20171\n",
      "Actual Positive               13335               10125\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.407819\n",
      "Recall           0.431586\n",
      "Precision (pos)  0.334203\n",
      "Precision (neg)  0.854227\n",
      "Specificity      0.794831\n"
     ]
    }
   ],
   "source": [
    "gnb_train_summary = my_func.classifier_summary(y_train, gnb_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               19676                4954\n",
      "Actual Positive                3362                2452\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.399843\n",
      "Recall           0.421741\n",
      "Precision (pos)  0.331083\n",
      "Precision (neg)  0.854067\n",
      "Specificity      0.798863\n"
     ]
    }
   ],
   "source": [
    "gnb_val_summary = my_func.classifier_summary(y_val, gnb_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The naive Bayes classifier performs worse than our OneR classifier in terms of F2 and recall. The class probabilities should already be accounted for by default, so it is possible this model is suffering from high dimensionality given our 40+ features. Additionally, with the large training set size of 120,000 records it is also possible the distinction between classes is muddled by the amount of overlapping data, however this seems less likely since both the k-fold cross validated and full training set scores are almost identical.\n",
    "\n",
    "Regardless, we will see if training set size shows any improvements:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_ind = np.random.choice(np.arange(0, X_train.shape[0], 1), size=20000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_sample = X_train[rand_ind]\n",
    "y_train_sample = y_train.iloc[rand_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb_s = GaussianNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "17.7 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "gnb_s_time = %timeit -n1 -r1 -o \\\n",
    "gnb_s.fit(X_train_sample, y_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb_s_f2_cvs = cross_val_score(estimator=gnb_s, X=X_train_sample, y=y_train_sample, cv=5, scoring=f2_score, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f2 score = 0.459 (+\\- 0.069)\n"
     ]
    }
   ],
   "source": [
    "my_func.print_cvs(gnb_s_f2_cvs, 'f2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "gnb_s_train_pred = gnb_s.predict(X_train)\n",
    "gnb_s_val_pred = gnb_s.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               75485               22829\n",
      "Actual Positive               12411               11049\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.432555\n",
      "Recall           0.470972\n",
      "Precision (pos)  0.326141\n",
      "Precision (neg)  0.858799\n",
      "Specificity      0.767795\n"
     ]
    }
   ],
   "source": [
    "gnb_train_summary = my_func.classifier_summary(y_train, gnb_s_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               19043                5587\n",
      "Actual Positive                3083                2731\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.432476\n",
      "Recall           0.469728\n",
      "Precision (pos)  0.328324\n",
      "Precision (neg)  0.860662\n",
      "Specificity      0.773163\n"
     ]
    }
   ],
   "source": [
    "gnb_val_summary = my_func.classifier_summary(y_val, gnb_s_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Very slight improvements are made with a smaller training size. Compared to our balanced class weight Logistic Regression and baseline OneR models, these results are still signficantly worse. However, it may be worth exploring the optimal training size further to maximize and implement GNB as via bagging."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### KNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn = Pipeline([('norm', Normalizer()),\n",
    "                ('estimator', KNeighborsClassifier(n_jobs=2))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "656 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "knn_time = %timeit -n1 -r1 -o \\\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_f2_cvs = cross_val_score(estimator=knn, X=X_train, y=y_train, cv=3, scoring=f2_score, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f2 score = 0.170 (+\\- 0.004)\n"
     ]
    }
   ],
   "source": [
    "my_func.print_cvs(knn_f2_cvs, 'f2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Already looking to be significantly worse than the other models tested until now, but this could also be a result of the training and/or fold sample sizes. Predictions using KNN are also likely to take some time due to its nature of having to process the entire set each time:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 6min 51s \n"
     ]
    }
   ],
   "source": [
    "my_func.run_time(reset=True)\n",
    "knn_train_pred = knn.predict(X_train)\n",
    "my_func.run_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1min 46s \n"
     ]
    }
   ],
   "source": [
    "my_func.run_time(reset=True)\n",
    "knn_val_pred = knn.predict(X_val)\n",
    "my_func.run_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               94861                3453\n",
      "Actual Positive               16437                7023\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.336621\n",
      "Recall           0.299361\n",
      "Precision (pos)  0.670389\n",
      "Precision (neg)  0.852315\n",
      "Specificity      0.964878\n"
     ]
    }
   ],
   "source": [
    "knn_train_summary =  my_func.classifier_summary(y_train, knn_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               22943                1687\n",
      "Actual Positive                4947                 867\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.167958\n",
      "Recall           0.149123\n",
      "Precision (pos)  0.339468\n",
      "Precision (neg)  0.822625\n",
      "Specificity      0.931506\n"
     ]
    }
   ],
   "source": [
    "knn_val_summary = my_func.classifier_summary(y_val, knn_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "KNN performs signficantly worse on the validation set. We will see if reducing the training size has a noticeable effect:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_ind = np.random.choice(np.arange(0, X_train.shape[0], 1), size=20000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_sample = X_train[rand_ind]\n",
    "y_train_sample = y_train.iloc[rand_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski',\n",
       "           metric_params=None, n_jobs=2, n_neighbors=5, p=2,\n",
       "           weights='uniform')"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn_s = KNeighborsClassifier(n_jobs=2)\n",
    "knn_s.fit(X_train_sample, y_train_sample)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_s_f2_cvs = cross_val_score(estimator=knn_s, X=X_train_sample, y=y_train_sample, cv=3, scoring=f2_score, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV score = 0.157 (+\\- 0.012)\n"
     ]
    }
   ],
   "source": [
    "my_func.print_cvs(knn_s_f2_cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "knn_s_val_pred = knn_s.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               23112                1518\n",
      "Actual Positive                5010                 804\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.157166\n",
      "Recall           0.138287\n",
      "Precision (pos)  0.346253\n",
      "Precision (neg)  0.821848\n",
      "Specificity      0.938368\n"
     ]
    }
   ],
   "source": [
    "my_func.classifier_summary(y_val, knn_s_val_pred);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Reducing the sample size (although just one scenario) shows a decrease in performance. Perhaps some parameter tuning may show improvements but relative to other model performances our F2 and recall scores are sigificantly worse. Failing to exceed scores in our OneR baseline, it is unlikely KNN will be able to out perform other models even with tuning. Consequently, KNN should be a low priority model for additional tuning, but it is still worth keeping in consideration in tandem with dimensionalty reductions methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support Vector Classifiers are likely to be computationally expensive due to the number of features, but we will still give it a try and see how it performs. We will begin with balanced class weight given our observations with Logistic Regression:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsvc = LinearSVC(class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24.4 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "lsvc_time = %timeit -n1 -r1 -o \\\n",
    "lsvc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Somewhat slower due to large number of training instances n (160k) and features m (59). (complexity $O(m\\times n)$)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsvc_f2_cvs = cross_val_score(estimator=lsvc, X=X_train, y=y_train, cv=5, scoring=f2_score, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean CV score = 0.533 (+\\- 0.005)\n"
     ]
    }
   ],
   "source": [
    "my_func.print_cvs(lsvc_f2_cvs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "lsvc_train_pred = lsvc.predict(X_train)\n",
    "lsvc_val_pred = lsvc.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               64420               33894\n",
      "Actual Positive                8249               15211\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.532058\n",
      "Recall           0.648380\n",
      "Precision (pos)  0.309765\n",
      "Precision (neg)  0.886485\n",
      "Specificity      0.655247\n"
     ]
    }
   ],
   "source": [
    "lsvc_train_summary = my_func.classifier_summary(y_train, lsvc_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 157,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               16191                8439\n",
      "Actual Positive                2042                3772\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.531762\n",
      "Recall           0.648779\n",
      "Precision (pos)  0.308902\n",
      "Precision (neg)  0.888005\n",
      "Specificity      0.657369\n"
     ]
    }
   ],
   "source": [
    "lsvc_val_summary = my_func.classifier_summary(y_val, lsvc_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "These results are very similar to those seen with our balanced Logistic Regression model. Compared to our OneR baseline, we see slight improvements to F2 due to higher precision (and specificty). Our recall is slightly below the baseline of 0.65, but with additional parameter tuning this can hopefully be improved. Consequently, Linear SVC shows promise and should be explored further."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Classifier - Polynomial"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial SVC will take an exceptionally long time to fit given the size our training set. However, fitting the model on a sample of the training data may at least give some insight into how the model compares to others. Noting the improvements acheived via balanced class weights we will proceed with the same setting here:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 160,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_ind = np.random.choice(np.arange(0, X_train.shape[0], 1), size=20000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_sample = X_train[rand_ind]\n",
    "y_train_sample = y_train.iloc[rand_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "psvc = SVC(kernel='poly', degree=2, class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "22.3 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "psvc_time = %timeit -n1 -r1 -o \\\n",
    "psvc.fit(X_train_sample, y_train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with 20,000 samples this is one of the slower fitting times yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 164,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "psvc_f2_cvs = cross_val_score(estimator=psvc, X=X_train_sample, y=y_train_sample, cv=5, scoring=f2_score, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f2 score = 0.532 (+\\- 0.016)\n"
     ]
    }
   ],
   "source": [
    "my_func.print_cvs(psvc_f2_cvs, 'f2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1min 6s \n"
     ]
    }
   ],
   "source": [
    "my_func.run_time(reset=True)\n",
    "psvc_train_pred = psvc.predict(X_train)\n",
    "my_func.run_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 16.49s \n"
     ]
    }
   ],
   "source": [
    "my_func.run_time(reset=True)\n",
    "psvc_val_pred = psvc.predict(X_val)\n",
    "my_func.run_time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even with the reduced sample these predictions take a considerable amount of time to process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 168,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               64484               33830\n",
      "Actual Positive                8199               15261\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.533859\n",
      "Recall           0.650512\n",
      "Precision (pos)  0.310872\n",
      "Precision (neg)  0.887195\n",
      "Specificity      0.655898\n"
     ]
    }
   ],
   "source": [
    "psvc_train_summary = my_func.classifier_summary(y_train, psvc_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               16156                8474\n",
      "Actual Positive                2084                3730\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.525945\n",
      "Recall           0.641555\n",
      "Precision (pos)  0.305637\n",
      "Precision (neg)  0.885746\n",
      "Specificity      0.655948\n"
     ]
    }
   ],
   "source": [
    "psvc_val_summary = my_func.classifier_summary(y_val, psvc_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This performance is almost identical to our linear SVC model. Let us see how it performs using the entire training set (will take a while):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 172,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 20min 13s \n"
     ]
    }
   ],
   "source": [
    "my_func.run_time(reset=True)\n",
    "psvc.fit(X_train, y_train)\n",
    "my_func.run_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 174,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 6min 40s \n"
     ]
    }
   ],
   "source": [
    "my_func.run_time(reset=True)\n",
    "psvc_train_pred = psvc.predict(X_train)\n",
    "my_func.run_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 173,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 1min 40s \n"
     ]
    }
   ],
   "source": [
    "my_func.run_time(reset=True)\n",
    "psvc_val_pred = psvc.predict(X_val)\n",
    "my_func.run_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               61118               37196\n",
      "Actual Positive                7022               16438\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.557319\n",
      "Recall           0.700682\n",
      "Precision (pos)  0.306485\n",
      "Precision (neg)  0.896947\n",
      "Specificity      0.621661\n"
     ]
    }
   ],
   "source": [
    "psvc_train_summary = my_func.classifier_summary(y_train, psvc_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               15276                9354\n",
      "Actual Positive                1777                4037\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.550795\n",
      "Recall           0.694358\n",
      "Precision (pos)  0.301471\n",
      "Precision (neg)  0.895795\n",
      "Specificity      0.620219\n"
     ]
    }
   ],
   "source": [
    "psvc_val_summary = my_func.classifier_summary(y_val, psvc_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The polynomial SVC (2nd degree) is actually the best performing model yet in terms of F2 and recall, whilst still maintaining an acceptable precision and specificity. However, considering the computational complexity and the resulting time to both fit and predict, exploring this model in depth is likely to be less feasible within the constraints of this project. Thus, delving into this model further should be a lower priority."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Support Vector Classifier  - RBF"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Continuing with SVC, we will see how the RBF kernel performs. Due to the computational costs we will start with a reduced training sample for an initial estimate on performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rand_ind = np.random.choice(np.arange(0, X_train.shape[0], 1), size=20000, replace=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train_sample = X_train[rand_ind]\n",
    "y_train_sample = y_train.iloc[rand_ind]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rsvc = SVC(kernel='rbf', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30.7 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n"
     ]
    }
   ],
   "source": [
    "rsvc_time = %timeit -n1 -r1 -o \\\n",
    "rsvc.fit(X_train_sample, y_train_sample)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparable fit time to our polynomial SVC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 181,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rsvc_f2_cvs = cross_val_score(estimator=rsvc, X=X_train_sample, y=y_train_sample, cv=5, scoring=f2_score, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f2 score = 0.533 (+\\- 0.012)\n"
     ]
    }
   ],
   "source": [
    "my_func.print_cvs(rsvc_f2_cvs, 'f2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rsvc_train_pred = rsvc.predict(X_train_sample)\n",
    "rsvc_val_pred = rsvc.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               10649                5489\n",
      "Actual Positive                1084                2778\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.585705\n",
      "Recall           0.719316\n",
      "Precision (pos)  0.336035\n",
      "Precision (neg)  0.907611\n",
      "Specificity      0.659871\n"
     ]
    }
   ],
   "source": [
    "rsvc_train_summary = my_func.classifier_summary(y_train_sample, rsvc_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               15981                8649\n",
      "Actual Positive                1977                3837\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.536763\n",
      "Recall           0.659959\n",
      "Precision (pos)  0.307304\n",
      "Precision (neg)  0.889910\n",
      "Specificity      0.648843\n"
     ]
    }
   ],
   "source": [
    "rsvc_val_summary = my_func.classifier_summary(y_val, rsvc_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "RBF performs slightly better than our polynomial SVC, albeit with a degree of underfitting with the validation set showing lower recall and precision. This may simply be a result of the sample size noting that your CV scores were lower. Given our success with polynomial SVC, we will attempt to fit the entire training set using RBF:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 179,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rsvc = SVC(kernel='rbf', class_weight='balanced')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 28min 21s \n"
     ]
    }
   ],
   "source": [
    "my_func.run_time(reset=True)\n",
    "rsvc.fit(X_train, y_train)\n",
    "my_func.run_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 9min 51s \n"
     ]
    }
   ],
   "source": [
    "my_func.run_time(reset=True)\n",
    "rsvc_train_pred = rsvc.predict(X_train)\n",
    "my_func.run_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Time: 2min 29s \n"
     ]
    }
   ],
   "source": [
    "my_func.run_time(reset=True)\n",
    "rsvc_val_pred = rsvc.predict(X_val)\n",
    "my_func.run_time()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               63674               34640\n",
      "Actual Positive                6974               16486\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.568616\n",
      "Recall           0.702728\n",
      "Precision (pos)  0.322458\n",
      "Precision (neg)  0.901285\n",
      "Specificity      0.647660\n"
     ]
    }
   ],
   "source": [
    "rsvc_train_summary = my_func.classifier_summary(y_train, rsvc_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               15872                8758\n",
      "Actual Positive                1879                3935\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.547303\n",
      "Recall           0.676815\n",
      "Precision (pos)  0.310013\n",
      "Precision (neg)  0.894147\n",
      "Specificity      0.644417\n"
     ]
    }
   ],
   "source": [
    "rsvc_val_summary = my_func.classifier_summary(y_val, rsvc_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comparible results to our polynomial SVC, with some slightly reduced recall and precision on the validation set. It is possible that the full training data set leads to suboptimal results. It may be worth further pursuing variations in training sizes along with parameter optimization. Additionally, whilst this model is quite computationally expensive, perhaps with feature and dimension reduction efforts both performance and accuracy can be improved."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our OneR baseline model is already a decision tree model, but perhaps some basic tuning can provide some immediate improvements. Firstly, we will evaluate a completely unconstrained decision tree to see how deep it extends:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=1, min_samples_split=2,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "46"
      ]
     },
     "execution_count": 196,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc.tree_.max_depth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The unconstrained decision tree has a depth of 46, but leaving this as is will almost certainly lead to overfitting. We will set up our decision tree with some basic contraints to limit overfitting:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtc?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtc = DecisionTreeClassifier(class_weight='balanced', max_depth=20, min_samples_split=100, min_samples_leaf=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight='balanced', criterion='gini',\n",
       "            max_depth=20, max_features=None, max_leaf_nodes=None,\n",
       "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
       "            min_samples_leaf=100, min_samples_split=100,\n",
       "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
       "            splitter='best')"
      ]
     },
     "execution_count": 234,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dtc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtc_f2_cvs = cross_val_score(estimator=dtc, X=X_train, y=y_train, cv=5, scoring=f2_score, n_jobs=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f2 score = 0.511 (+\\- 0.006)\n"
     ]
    }
   ],
   "source": [
    "my_func.print_cvs(dtc_f2_cvs, 'f2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "dtc_train_pred = dtc.predict(X_train)\n",
    "dtc_val_pred = dtc.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               63287               35027\n",
      "Actual Positive                6258               17202\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.588831\n",
      "Recall           0.733248\n",
      "Precision (pos)  0.329357\n",
      "Precision (neg)  0.910015\n",
      "Specificity      0.643723\n"
     ]
    }
   ],
   "source": [
    "dtc_train_summary = my_func.classifier_summary(y_train, dtc_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               15371                9259\n",
      "Actual Positive                2108                3706\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.511582\n",
      "Recall           0.637427\n",
      "Precision (pos)  0.285847\n",
      "Precision (neg)  0.879398\n",
      "Specificity      0.624076\n"
     ]
    }
   ],
   "source": [
    "dtc_val_summary = my_func.classifier_summary(y_val, dtc_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given the difference between our training scores and CV / validation scores, this decision tree is clearly still suffering from  overfitting. Regardless, our validation scores are still within the acceptable ranges, and considering we arbitrarily chose certin constraints there should be room for improvement with tuning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Random Forest Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forests are simply collections of decision trees, so we can carry forward what we have learned so far with our initial model, but will constrain our tree a little more for the ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc = RandomForestClassifier(n_estimators=20, max_depth=10, min_samples_split=100, min_samples_leaf=100, class_weight='balanced', n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "RandomForestClassifier(bootstrap=True, class_weight='balanced',\n",
       "            criterion='gini', max_depth=10, max_features='auto',\n",
       "            max_leaf_nodes=None, min_impurity_decrease=0.0,\n",
       "            min_impurity_split=None, min_samples_leaf=100,\n",
       "            min_samples_split=100, min_weight_fraction_leaf=0.0,\n",
       "            n_estimators=20, n_jobs=2, oob_score=False, random_state=None,\n",
       "            verbose=0, warm_start=False)"
      ]
     },
     "execution_count": 250,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rfc.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc_f2_cvs = cross_val_score(estimator=rfc, X=X_train, y=y_train, cv=5, scoring=f2_score, n_jobs=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean f2 score = 0.531 (+\\- 0.006)\n"
     ]
    }
   ],
   "source": [
    "my_func.print_cvs(rfc_f2_cvs, 'f2')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This CV score is a promising start. Let us see how it fares in terms of train vs. validation scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "rfc_train_pred = rfc.predict(X_train)\n",
    "rfc_val_pred = rfc.predict(X_val)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               64978               33336\n",
      "Actual Positive                7380               16080\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.561233\n",
      "Recall           0.685422\n",
      "Precision (pos)  0.325401\n",
      "Precision (neg)  0.898007\n",
      "Specificity      0.660923\n"
     ]
    }
   ],
   "source": [
    "rfc_train_summary = my_func.classifier_summary(y_train, rfc_train_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Confusion Matrix:\n",
      "                 Predicted Negative  Predicted Positive\n",
      "Actual Negative               16126                8504\n",
      "Actual Positive                2054                3760\n",
      "--------------------\n",
      "Accuracy Scores:\n",
      "                    Score\n",
      "Rate                     \n",
      "F2               0.529279\n",
      "Recall           0.646715\n",
      "Precision (pos)  0.306588\n",
      "Precision (neg)  0.887019\n",
      "Specificity      0.654730\n"
     ]
    }
   ],
   "source": [
    "rfc_val_summary = my_func.classifier_summary(y_val, rfc_val_pred)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Random forest shows slight improvement over individual decisions trees in terms of overfitting. Whilst Logistic Regression and SVC have peformed better thus far, given the speed at which these models can be tested and the general robustness of random forests, Random Forests should definitely be explored further."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
